{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\sansk\\anaconda3\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\sansk\\anaconda3\\lib\\site-packages (from opencv-python) (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# Step 5: Create a loop for real-time processing\n",
    "with mp_hands.Hands(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "        success, image = cap.read()\n",
    "\n",
    "# Step 6: Inside the loop, read frames from the webcam\n",
    "\n",
    "# Step 7: Convert and flip the image:\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Step 8: Process the image using MediaPipe Hands\n",
    "        results = hands.process(image)\n",
    "\n",
    "# Step 9: Draw hand landmarks on the image\n",
    "if results.multi_hand_landmarks:\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "\n",
    "# Step 10: Display the annotated ima\n",
    "cv2.imshow('MediaPipe Hands', image)\n",
    "\n",
    "# Step 11: Break the loop if the 'Esc' key is pressed\n",
    "# if cv2.waitKey(5) & 0xFF == 27:\n",
    "#     break\n",
    "\n",
    "# Step 12: Release the webcam and close the window when done\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      9\u001b[0m sns\u001b[38;5;241m.\u001b[39mset()\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CatBoostRegressor, Pool, cv\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MetricVisualizer\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeSeriesSplit\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "## Warehouse optimization\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool, cv\n",
    "from catboost import MetricVisualizer\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "from os import listdir\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "data = pd.read_csv(\"../input/ecommerce-data/data.csv\", encoding=\"ISO-8859-1\", dtype={'CustomerID': str})\n",
    "data.shape\n",
    "\n",
    "data[data.Description.isnull()].head()\n",
    "data[data.Description.isnull()].CustomerID.isnull().value_counts()\n",
    "data[data.Description.isnull()].UnitPrice.value_counts()\n",
    "data[data.CustomerID.isnull()].head()\n",
    "data.loc[data.CustomerID.isnull(), [\"UnitPrice\", \"Quantity\"]].describe()\n",
    "\n",
    "data.loc[data.Description.isnull()==False, \"lowercase_descriptions\"] = data.loc[\n",
    "    data.Description.isnull()==False,\"Description\"\n",
    "].apply(lambda l: l.lower())\n",
    "\n",
    "data.lowercase_descriptions.dropna().apply(\n",
    "    lambda l: np.where(\"nan\" in l, True, False)\n",
    ").value_counts()\n",
    "data.lowercase_descriptions.dropna().apply(\n",
    "    lambda l: np.where(\"\" == l, True, False)\n",
    ").value_counts()\n",
    "data.loc[data.lowercase_descriptions.isnull()==False, \"lowercase_descriptions\"] = data.loc[\n",
    "    data.lowercase_descriptions.isnull()==False, \"lowercase_descriptions\"\n",
    "].apply(lambda l: np.where(\"nan\" in l, None, l))\n",
    "data = data.loc[(data.CustomerID.isnull()==False) & (data.lowercase_descriptions.isnull()==False)].copy()\n",
    "data.isnull().sum().sum()\n",
    "### The Time period <a class=\"anchor\" id=\"timeperiod\"></a>\n",
    "\n",
    "data[\"InvoiceDate\"] = pd.to_datetime(data.InvoiceDate, cache=True)\n",
    "\n",
    "data.InvoiceDate.max() - data.InvoiceDate.min()\n",
    "print(\"Datafile starts with timepoint {}\".format(data.InvoiceDate.min()))\n",
    "print(\"Datafile ends with timepoint {}\".format(data.InvoiceDate.max()))\n",
    "### The invoice number <a class=\"anchor\" id=\"invoiceno\"></a>\n",
    "data.InvoiceNo.nunique()\n",
    "data[\"IsCancelled\"]=np.where(data.InvoiceNo.apply(lambda l: l[0]==\"C\"), True, False)\n",
    "data.IsCancelled.value_counts() / data.shape[0] * 100\n",
    "data.loc[data.IsCancelled==True].describe()\n",
    "data = data.loc[data.IsCancelled==False].copy()\n",
    "data = data.drop(\"IsCancelled\", axis=1)\n",
    "### Stockcodes <a class=\"anchor\" id=\"stockcodes\"></a>\n",
    "data.StockCode.nunique()\n",
    "stockcode_counts = data.StockCode.value_counts().sort_values(ascending=False)\n",
    "fig, ax = plt.subplots(2,1,figsize=(20,15))\n",
    "sns.barplot(stockcode_counts.iloc[0:20].index,\n",
    "            stockcode_counts.iloc[0:20].values,\n",
    "            ax = ax[0], palette=\"Oranges_r\")\n",
    "ax[0].set_ylabel(\"Counts\")\n",
    "ax[0].set_xlabel(\"Stockcode\")\n",
    "ax[0].set_title(\"Which stockcodes are most common?\");\n",
    "sns.distplot(np.round(stockcode_counts/data.shape[0]*100,2),\n",
    "             kde=False,\n",
    "             bins=20,\n",
    "             ax=ax[1], color=\"Orange\")\n",
    "ax[1].set_title(\"How seldom are stockcodes?\")\n",
    "ax[1].set_xlabel(\"% of data with this stockcode\")\n",
    "ax[1].set_ylabel(\"Frequency\");\n",
    "def count_numeric_chars(l):\n",
    "    return sum(1 for c in l if c.isdigit())\n",
    "\n",
    "data[\"StockCodeLength\"] = data.StockCode.apply(lambda l: len(l))\n",
    "data[\"nNumericStockCode\"] = data.StockCode.apply(lambda l: count_numeric_chars(l))\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "sns.countplot(data[\"StockCodeLength\"], palette=\"Oranges_r\", ax=ax[0])\n",
    "sns.countplot(data[\"nNumericStockCode\"], palette=\"Oranges_r\", ax=ax[1])\n",
    "ax[0].set_xlabel(\"Length of stockcode\")\n",
    "ax[1].set_xlabel(\"Number of numeric chars in the stockcode\");\n",
    "data.loc[data.nNumericStockCode < 5].lowercase_descriptions.value_counts()\n",
    "data = data.loc[(data.nNumericStockCode == 5) & (data.StockCodeLength==5)].copy()\n",
    "data.StockCode.nunique()\n",
    "data = data.drop([\"nNumericStockCode\", \"StockCodeLength\"], axis=1)\n",
    "data.Description.nunique()\n",
    "description_counts = data.Description.value_counts().sort_values(ascending=False).iloc[0:30]\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.barplot(description_counts.index, description_counts.values, palette=\"Purples_r\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Which product descriptions are most common?\");\n",
    "plt.xticks(rotation=90);\n",
    "def count_lower_chars(l):\n",
    "    return sum(1 for c in l if c.islower())\n",
    "data[\"DescriptionLength\"] = data.Description.apply(lambda l: len(l))\n",
    "data[\"LowCharsInDescription\"] = data.Description.apply(lambda l: count_lower_chars(l))\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "sns.countplot(data.DescriptionLength, ax=ax[0], color=\"Purple\")\n",
    "sns.countplot(data.LowCharsInDescription, ax=ax[1], color=\"Purple\")\n",
    "ax[1].set_yscale(\"log\")\n",
    "lowchar_counts = data.loc[data.LowCharsInDescription > 0].Description.value_counts()\n",
    "\n",
    "plt.figure(figsize=(15,3))\n",
    "sns.barplot(lowchar_counts.index, lowchar_counts.values, palette=\"Purples_r\")\n",
    "plt.xticks(rotation=90);\n",
    "def count_upper_chars(l):\n",
    "    return sum(1 for c in l if c.isupper())\n",
    "\n",
    "data[\"UpCharsInDescription\"] = data.Description.apply(lambda l: count_upper_chars(l))\n",
    "data.UpCharsInDescription.describe()\n",
    "data.loc[data.UpCharsInDescription <=5].Description.value_counts()\n",
    "data = data.loc[data.UpCharsInDescription > 5].copy()\n",
    "dlength_counts = data.loc[data.DescriptionLength < 14].Description.value_counts()\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.barplot(dlength_counts.index, dlength_counts.values, palette=\"Purples_r\")\n",
    "plt.xticks(rotation=90);\n",
    "data.StockCode.nunique()\n",
    "data.Description.nunique()\n",
    "data.groupby(\"StockCode\").Description.nunique().sort_values(ascending=False).iloc[0:10]\n",
    "data.loc[data.StockCode == \"23244\"].Description.value_counts()\n",
    "data.CustomerID.nunique()\n",
    "customer_counts = data.CustomerID.value_counts().sort_values(ascending=False).iloc[0:20] \n",
    "plt.figure(figsize=(20,5))\n",
    "sns.barplot(customer_counts.index, customer_counts.values, order=customer_counts.index)\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.xlabel(\"CustomerID\")\n",
    "plt.title(\"Which customers are most common?\");\n",
    "#plt.xticks(rotation=90);\n",
    "### Countries <a class=\"anchor\" id=\"countries\"></a>\n",
    "\n",
    "data.Country.nunique()\n",
    "country_counts = data.Country.value_counts().sort_values(ascending=False).iloc[0:20]\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.barplot(country_counts.index, country_counts.values, palette=\"Greens_r\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Which countries made the most transactions?\");\n",
    "plt.xticks(rotation=90);\n",
    "plt.yscale(\"log\")\n",
    "data.loc[data.Country==\"United Kingdom\"].shape[0] / data.shape[0] * 100\n",
    "data[\"UK\"] = np.where(data.Country == \"United Kingdom\", 1, 0)\n",
    "### Unit Price <a class=\"anchor\" id=\"unitprice\"></a>\n",
    "data.UnitPrice.describe()\n",
    "data.loc[data.UnitPrice == 0].sort_values(by=\"Quantity\", ascending=False).head()\n",
    "data = data.loc[data.UnitPrice > 0].copy()\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "sns.distplot(data.UnitPrice, ax=ax[0], kde=False, color=\"red\")\n",
    "sns.distplot(np.log(data.UnitPrice), ax=ax[1], bins=20, color=\"tomato\", kde=False)\n",
    "ax[1].set_xlabel(\"Log-Unit-Price\");\n",
    "np.exp(-2)\n",
    "np.exp(3)\n",
    "np.quantile(data.UnitPrice, 0.95)\n",
    "data = data.loc[(data.UnitPrice > 0.1) & (data.UnitPrice < 20)].copy()\n",
    "### Quantities <a class=\"anchor\" id=\"quantities\"></a>\n",
    "\n",
    "data.Quantity.describe()\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "sns.distplot(data.Quantity, ax=ax[0], kde=False, color=\"limegreen\");\n",
    "sns.distplot(np.log(data.Quantity), ax=ax[1], bins=20, kde=False, color=\"limegreen\");\n",
    "ax[0].set_title(\"Quantity distribution\")\n",
    "ax[0].set_yscale(\"log\")\n",
    "ax[1].set_title(\"Log-Quantity distribution\")\n",
    "ax[1].set_xlabel(\"Natural-Log Quantity\");\n",
    "np.exp(4)\n",
    "np.quantile(data.Quantity, 0.95)\n",
    "data = data.loc[data.Quantity < 55].copy()\n",
    "\n",
    "data[\"Revenue\"] = data.Quantity * data.UnitPrice\n",
    "\n",
    "data[\"Year\"] = data.InvoiceDate.dt.year\n",
    "data[\"Quarter\"] = data.InvoiceDate.dt.quarter\n",
    "data[\"Month\"] = data.InvoiceDate.dt.month\n",
    "data[\"Week\"] = data.InvoiceDate.dt.week\n",
    "data[\"Weekday\"] = data.InvoiceDate.dt.weekday\n",
    "data[\"Day\"] = data.InvoiceDate.dt.day\n",
    "data[\"Dayofyear\"] = data.InvoiceDate.dt.dayofyear\n",
    "data[\"Date\"] = pd.to_datetime(data[['Year', 'Month', 'Day']])\n",
    "grouped_features = [\"Date\", \"Year\", \"Quarter\",\"Month\", \"Week\", \"Weekday\", \"Dayofyear\", \"Day\",\n",
    "                    \"StockCode\"]\n",
    "daily_data = pd.DataFrame(data.groupby(grouped_features).Quantity.sum(),\n",
    "                          columns=[\"Quantity\"])\n",
    "daily_data[\"Revenue\"] = data.groupby(grouped_features).Revenue.sum()\n",
    "daily_data = daily_data.reset_index()\n",
    "daily_data.head(5)\n",
    "daily_data.loc[:, [\"Quantity\", \"Revenue\"]].describe()\n",
    "low_quantity = daily_data.Quantity.quantile(0.01)\n",
    "high_quantity = daily_data.Quantity.quantile(0.99)\n",
    "print((low_quantity, high_quantity))\n",
    "low_revenue = daily_data.Revenue.quantile(0.01)\n",
    "high_revenue = daily_data.Revenue.quantile(0.99)\n",
    "print((low_revenue, high_revenue))\n",
    "samples = daily_data.shape[0]\n",
    "daily_data = daily_data.loc[\n",
    "    (daily_data.Quantity >= low_quantity) & (daily_data.Quantity <= high_quantity)]\n",
    "daily_data = daily_data.loc[\n",
    "    (daily_data.Revenue >= low_revenue) & (daily_data.Revenue <= high_revenue)]\n",
    "samples - daily_data.shape[0]\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "sns.distplot(daily_data.Quantity.values, kde=True, ax=ax[0], color=\"Orange\", bins=30);\n",
    "sns.distplot(np.log(daily_data.Quantity.values), kde=True, ax=ax[1], color=\"Orange\", bins=30);\n",
    "ax[0].set_xlabel(\"Number of daily product sales\");\n",
    "ax[0].set_ylabel(\"Frequency\");\n",
    "ax[0].set_title(\"How many products are sold per day?\");\n",
    "## How to predict daily product sales? <a class=\"anchor\" id=\"model\"></a>\n",
    "\n",
    "\n",
    "# $$ E = \\sqrt{ \\frac{1}{N}\\sum_{n=1}^{N} (t_{n} - y_{n})^{2}}$$\n",
    "\n",
    "\n",
    "class CatHyperparameter:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 loss=\"RMSE\",\n",
    "                 metric=\"RMSE\",\n",
    "                 iterations=1000,\n",
    "                 max_depth=4,\n",
    "                 l2_leaf_reg=3,\n",
    "                 #learning_rate=0.5,\n",
    "                 seed=0):\n",
    "        self.loss = loss,\n",
    "        self.metric = metric,\n",
    "        self.max_depth = max_depth,\n",
    "        self.l2_leaf_reg = l2_leaf_reg,\n",
    "        #self.learning_rate = learning_rate,\n",
    "        self.iterations=iterations\n",
    "        self.seed = seed\n",
    "class Catmodel:\n",
    "    \n",
    "    def __init__(self, name, params):\n",
    "        self.name = name\n",
    "        self.params = params\n",
    "    \n",
    "    def set_data_pool(self, train_pool, val_pool):\n",
    "        self.train_pool = train_pool\n",
    "        self.val_pool = val_pool\n",
    "    \n",
    "    def set_data(self, X, y, week):\n",
    "        cat_features_idx = np.where(X.dtypes != np.float)[0]\n",
    "        x_train, self.x_val = X.loc[X.Week < week], X.loc[X.Week >= week]\n",
    "        y_train, self.y_val = y.loc[X.Week < week], y.loc[X.Week >= week]\n",
    "        self.train_pool = Pool(x_train, y_train, cat_features=cat_features_idx)\n",
    "        self.val_pool = Pool(self.x_val, self.y_val, cat_features=cat_features_idx)\n",
    "    \n",
    "    def prepare_model(self):\n",
    "        self.model = CatBoostRegressor(\n",
    "                loss_function = self.params.loss[0],\n",
    "                random_seed = self.params.seed,\n",
    "                logging_level = 'Silent',\n",
    "                iterations = self.params.iterations,\n",
    "                max_depth = self.params.max_depth[0],\n",
    "                #learning_rate = self.params.learning_rate[0],\n",
    "                l2_leaf_reg = self.params.l2_leaf_reg[0],\n",
    "                od_type='Iter',\n",
    "                od_wait=40,\n",
    "                train_dir=self.name,\n",
    "                has_time=True\n",
    "            )\n",
    "    \n",
    "    def learn(self, plot=False):\n",
    "        self.prepare_model()\n",
    "        self.model.fit(self.train_pool, eval_set=self.val_pool, plot=plot);\n",
    "        print(\"{}, early-stopped model tree count {}\".format(\n",
    "            self.name, self.model.tree_count_\n",
    "        ))\n",
    "    \n",
    "    def score(self):\n",
    "        return self.model.score(self.val_pool)\n",
    "    \n",
    "    def show_importances(self, kind=\"bar\"):\n",
    "        explainer = shap.TreeExplainer(self.model)\n",
    "        shap_values = explainer.shap_values(self.val_pool)\n",
    "        if kind==\"bar\":\n",
    "            return shap.summary_plot(shap_values, self.x_val, plot_type=\"bar\")\n",
    "        return shap.summary_plot(shap_values, self.x_val)\n",
    "    \n",
    "    def get_val_results(self):\n",
    "        self.results = pd.DataFrame(self.y_val)\n",
    "        self.results[\"prediction\"] = self.predict(self.x_val)\n",
    "        self.results[\"error\"] = np.abs(\n",
    "            self.results[self.results.columns.values[0]].values - self.results.prediction)\n",
    "        self.results[\"Month\"] = self.x_val.Month\n",
    "        self.results[\"SquaredError\"] = self.results.error.apply(lambda l: np.power(l, 2))\n",
    "    \n",
    "    def show_val_results(self):\n",
    "        self.get_val_results()\n",
    "        fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "        sns.distplot(self.results.error, ax=ax[0])\n",
    "        ax[0].set_xlabel(\"Single absolute error\")\n",
    "        ax[0].set_ylabel(\"Density\")\n",
    "        self.median_absolute_error = np.median(self.results.error)\n",
    "        print(\"Median absolute error: {}\".format(self.median_absolute_error))\n",
    "        ax[0].axvline(self.median_absolute_error, c=\"black\")\n",
    "        ax[1].scatter(self.results.prediction.values,\n",
    "                      self.results[self.results.columns[0]].values,\n",
    "                      c=self.results.error, cmap=\"RdYlBu_r\", s=1)\n",
    "        ax[1].set_xlabel(\"Prediction\")\n",
    "        ax[1].set_ylabel(\"Target\")\n",
    "        return ax\n",
    "    \n",
    "    def get_monthly_RMSE(self):\n",
    "        return self.results.groupby(\"Month\").SquaredError.mean().apply(lambda l: np.sqrt(l))\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "    \n",
    "    def get_dependence_plot(self, feature1, feature2=None):\n",
    "        explainer = shap.TreeExplainer(self.model)\n",
    "        shap_values = explainer.shap_values(self.val_pool)\n",
    "        if feature2 is None:\n",
    "            return shap.dependence_plot(\n",
    "                feature1,\n",
    "                shap_values,\n",
    "                self.x_val,\n",
    "            )\n",
    "        else:\n",
    "            return shap.dependence_plot(\n",
    "                feature1,\n",
    "                shap_values,\n",
    "                self.x_val,\n",
    "                interaction_index=feature2\n",
    "            )\n",
    "    \n",
    "    \n",
    "import GPyOpt\n",
    "\n",
    "class Hypertuner:\n",
    "    \n",
    "    def __init__(self, model, max_iter=10, max_time=10,max_depth=6, max_l2_leaf_reg=20):\n",
    "        self.bounds = [{'name': 'depth','type': 'discrete','domain': (1,max_depth)},\n",
    "                       {'name': 'l2_leaf_reg','type': 'discrete','domain': (1,max_l2_leaf_reg)}]\n",
    "        self.model = model\n",
    "        self.max_iter=max_iter\n",
    "        self.max_time=max_time\n",
    "        self.best_depth = None\n",
    "        self.best_l2_leaf_reg = None\n",
    "    \n",
    "    def objective(self, params):\n",
    "        params = params[0]\n",
    "        params = CatHyperparameter(\n",
    "            max_depth=params[0],\n",
    "            l2_leaf_reg=params[1]\n",
    "        )\n",
    "        self.model.params = params\n",
    "        self.model.learn()\n",
    "        return self.model.score()\n",
    "    \n",
    "    def learn(self):\n",
    "        np.random.seed(777)\n",
    "        optimizer = GPyOpt.methods.BayesianOptimization(\n",
    "            f=self.objective, domain=self.bounds,\n",
    "            acquisition_type ='EI',\n",
    "            acquisition_par = 0.2,\n",
    "            exact_eval=True)\n",
    "        optimizer.run_optimization(self.max_iter, self.max_time)\n",
    "        optimizer.plot_convergence()\n",
    "        best = optimizer.X[np.argmin(optimizer.Y)]\n",
    "        self.best_depth = best[0]\n",
    "        self.best_l2_leaf_reg = best[1]\n",
    "        print(\"Optimal depth is {} and optimal l2-leaf-reg is {}\".format(self.best_depth, self.best_l2_leaf_reg))\n",
    "        print('Optimal RMSE:', np.min(optimizer.Y))\n",
    "    \n",
    "    def retrain_catmodel(self):\n",
    "        params = CatHyperparameter(\n",
    "            max_depth=self.best_depth,\n",
    "            l2_leaf_reg=self.best_l2_leaf_reg\n",
    "        )\n",
    "        self.model.params = params\n",
    "        self.model.learn(plot=True)\n",
    "        return self.model\n",
    "class CatFamily:\n",
    "    \n",
    "    def __init__(self, params, X, y, n_splits=2):\n",
    "        self.family = {}\n",
    "        self.cat_features_idx = np.where(X.dtypes != np.float)[0]\n",
    "        self.X = X.values\n",
    "        self.y = y.values\n",
    "        self.n_splits = n_splits\n",
    "        self.params = params\n",
    "    \n",
    "    def set_validation_strategy(self):\n",
    "        self.cv = TimeSeriesSplit(max_train_size = None,\n",
    "                                  n_splits = self.n_splits)\n",
    "        self.gen = self.cv.split(self.X)\n",
    "    \n",
    "    def get_split(self):\n",
    "        train_idx, val_idx = next(self.gen)\n",
    "        x_train, x_val = self.X[train_idx], self.X[val_idx]\n",
    "        y_train, y_val = self.y[train_idx], self.y[val_idx]\n",
    "        train_pool = Pool(x_train, y_train, cat_features=self.cat_features_idx)\n",
    "        val_pool = Pool(x_val, y_val, cat_features=self.cat_features_idx)\n",
    "        return train_pool, val_pool\n",
    "    \n",
    "    def learn(self):\n",
    "        self.set_validation_strategy()\n",
    "        self.model_names = []\n",
    "        self.model_scores = []\n",
    "        for split in range(self.n_splits):\n",
    "            name = 'Model_cv_' + str(split) + '/'\n",
    "            train_pool, val_pool = self.get_split()\n",
    "            self.model_names.append(name)\n",
    "            self.family[name], score = self.fit_catmodel(name, train_pool, val_pool)\n",
    "            self.model_scores.append(score)\n",
    "    \n",
    "    def fit_catmodel(self, name, train_pool, val_pool):\n",
    "        cat = Catmodel(name, train_pool, val_pool, self.params)\n",
    "        cat.prepare_model()\n",
    "        cat.learn()\n",
    "        score = cat.score()\n",
    "        return cat, score\n",
    "    \n",
    "    def score(self):\n",
    "        return np.mean(self.model_scores)\n",
    "    \n",
    "    def show_learning(self):\n",
    "        widget = MetricVisualizer(self.model_names)\n",
    "        widget.start()\n",
    "\n",
    "    def show_importances(self):\n",
    "        name = self.model_names[-1]\n",
    "        cat = self.family[name]\n",
    "        explainer = shap.TreeExplainer(cat.model)\n",
    "        shap_values = explainer.shap_values(cat.val_pool)\n",
    "        return shap.summary_plot(shap_values, X, plot_type=\"bar\")\n",
    "daily_data.head()\n",
    "week = daily_data.Week.max() - 2\n",
    "print(\"Validation after week {}\".format(week))\n",
    "print(\"Validation starts at timepoint {}\".format(\n",
    "    daily_data[daily_data.Week==week].Date.min()\n",
    "))\n",
    "X = daily_data.drop([\"Quantity\", \"Revenue\", \"Date\"], axis=1)\n",
    "daily_data.Quantity = np.log(daily_data.Quantity)\n",
    "y = daily_data.Quantity\n",
    "params = CatHyperparameter()\n",
    "\n",
    "model = Catmodel(\"baseline\", params)\n",
    "model.set_data(X,y, week)\n",
    "model.learn(plot=True)\n",
    "model.score()\n",
    "model.show_val_results();\n",
    "model.show_importances()\n",
    "model.show_importances(kind=None)\n",
    "np.mean(np.abs(np.exp(model.results.prediction) - np.exp(model.results.Quantity)))\n",
    "np.median(np.abs(np.exp(model.results.prediction) - np.exp(model.results.Quantity)))\n",
    "products = pd.DataFrame(index=data.loc[data.Week < week].StockCode.unique(), columns = [\"MedianPrice\"])\n",
    "\n",
    "products[\"MedianPrice\"] = data.loc[data.Week < week].groupby(\"StockCode\").UnitPrice.median()\n",
    "products[\"MedianQuantities\"] = data.loc[data.Week < week].groupby(\"StockCode\").Quantity.median()\n",
    "products[\"Customers\"] = data.loc[data.Week < week].groupby(\"StockCode\").CustomerID.nunique()\n",
    "products[\"DescriptionLength\"] = data.loc[data.Week < week].groupby(\"StockCode\").DescriptionLength.median()\n",
    "#products[\"StockCode\"] = products.index.values\n",
    "org_cols = np.copy(products.columns.values)\n",
    "products.head()\n",
    "for col in org_cols:\n",
    "    if col != \"StockCode\":\n",
    "        products[col] = boxcox(products[col])[0]\n",
    "fig, ax = plt.subplots(1,3,figsize=(20,5))\n",
    "ax[0].scatter(products.MedianPrice.values, products.MedianQuantities.values,\n",
    "           c=products.Customers.values, cmap=\"coolwarm_r\")\n",
    "ax[0].set_xlabel(\"Boxcox-Median-UnitPrice\")\n",
    "ax[0].set_ylabel(\"Boxcox-Median-Quantities\")\n",
    "X = products.values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "km = KMeans(n_clusters=30)\n",
    "products[\"cluster\"] = km.fit_predict(X)\n",
    "\n",
    "daily_data[\"ProductType\"] = daily_data.StockCode.map(products.cluster)\n",
    "daily_data.ProductType = daily_data.ProductType.astype(\"object\")\n",
    "daily_data.head()\n",
    "## Baseline for product types\n",
    "daily_data[\"KnownStockCodeUnitPriceMedian\"] = daily_data.StockCode.map(\n",
    "    data.groupby(\"StockCode\").UnitPrice.median())\n",
    "\n",
    "known_price_iqr = data.groupby(\"StockCode\").UnitPrice.quantile(0.75) \n",
    "known_price_iqr -= data.groupby(\"StockCode\").UnitPrice.quantile(0.25) \n",
    "daily_data[\"KnownStockCodeUnitPriceIQR\"] = daily_data.StockCode.map(known_price_iqr)\n",
    "to_group = [\"StockCode\", \"Year\", \"Month\", \"Week\", \"Weekday\"]\n",
    "\n",
    "daily_data = daily_data.set_index(to_group)\n",
    "daily_data[\"KnownStockCodePrice_WW_median\"] = daily_data.index.map(\n",
    "    data.groupby(to_group).UnitPrice.median())\n",
    "daily_data[\"KnownStockCodePrice_WW_mean\"] = daily_data.index.map(\n",
    "    data.groupby(to_group).UnitPrice.mean().apply(lambda l: np.round(l, 2)))\n",
    "daily_data[\"KnownStockCodePrice_WW_std\"] = daily_data.index.map(\n",
    "    data.groupby(to_group).UnitPrice.std().apply(lambda l: np.round(l, 2)))\n",
    "\n",
    "daily_data = daily_data.reset_index()\n",
    "daily_data.head()\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(daily_data.groupby(\"Date\").Quantity.sum(), marker='+', c=\"darkorange\")\n",
    "plt.plot(daily_data.groupby(\"Date\").Quantity.sum().rolling(window=30, center=True).mean(),\n",
    "        c=\"red\")\n",
    "plt.xticks(rotation=90);\n",
    "plt.title(\"How many quantities are sold per day over the given time?\");\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "\n",
    "weekdays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "yearmonth = [\"Dec-2010\", \"Jan-2011\", \"Feb-2011\", \"Mar-2011\", \"Apr-2011\", \"May-2011\",\n",
    "             \"Jun-2011\", \"Jul-1011\", \"Aug-2011\", \"Sep-2011\", \"Oct-2011\", \"Nov-2011\", \n",
    "             \"Dec-2011\"]\n",
    "\n",
    "daily_data.groupby(\"Weekday\").Quantity.sum().plot(\n",
    "    ax=ax[0], marker='o', label=\"Quantity\", c=\"darkorange\");\n",
    "ax[0].legend();\n",
    "ax[0].set_xticks(np.arange(0,7))\n",
    "ax[0].set_xticklabels(weekdays);\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[0].set_title(\"Total sales per weekday\");\n",
    "\n",
    "ax[1].plot(daily_data.groupby([\"Year\", \"Month\"]).Quantity.sum().values,\n",
    "    marker='o', label=\"Quantities\", c=\"darkorange\");\n",
    "ax[1].set_xticklabels(yearmonth, rotation=90)\n",
    "ax[1].set_xticks(np.arange(0, len(yearmonth)))\n",
    "ax[1].legend();\n",
    "ax[1].set_title(\"Total sales per month\");\n",
    "daily_data[\"PreChristmas\"] = (daily_data.Dayofyear <= 358) & (daily_data.Dayofyear >= 243) \n",
    "for col in [\"Weekday\", \"Month\", \"Quarter\"]:\n",
    "    daily_data = daily_data.set_index(col)\n",
    "    daily_data[col+\"Quantity_mean\"] = daily_data.loc[daily_data.Week < week].groupby(col).Quantity.mean()\n",
    "    daily_data[col+\"Quantity_median\"] = daily_data.loc[daily_data.Week < week].groupby(col).Quantity.median()\n",
    "    daily_data[col+\"Quantity_mean_median_diff\"] = daily_data[col+\"Quantity_mean\"] - daily_data[col+\"Quantity_median\"]\n",
    "    daily_data[col+\"Quantity_IQR\"] = daily_data.loc[\n",
    "        daily_data.Week < week].groupby(col).Quantity.quantile(0.75) - daily_data.loc[\n",
    "        daily_data.Week < week].groupby(col).Quantity.quantile(0.25)\n",
    "    daily_data = daily_data.reset_index()\n",
    "daily_data.head()\n",
    "to_group = [\"StockCode\", \"PreChristmas\"]\n",
    "daily_data = daily_data.set_index(to_group)\n",
    "daily_data[\"PreChristmasMeanQuantity\"] = daily_data.loc[\n",
    "    daily_data.Week < week].groupby(to_group).Quantity.mean().apply(lambda l: np.round(l, 1))\n",
    "daily_data[\"PreChristmasMedianQuantity\"] = daily_data.loc[\n",
    "    daily_data.Week < week].groupby(to_group).Quantity.median().apply(lambda l: np.round(l, 1))\n",
    "daily_data[\"PreChristmasStdQuantity\"] = daily_data.loc[\n",
    "    daily_data.Week < week].groupby(to_group).Quantity.std().apply(lambda l: np.round(l, 1))\n",
    "daily_data = daily_data.reset_index()\n",
    "\n",
    "for delta in range(1,4):\n",
    "    to_group = [\"Week\",\"Weekday\",\"ProductType\"]\n",
    "    daily_data = daily_data.set_index(to_group)\n",
    "        \n",
    "    daily_data[\"QuantityProducttypeWeekWeekdayLag_\" + str(delta) + \"_median\"] = daily_data.groupby(\n",
    "        to_group).Quantity.median().apply(lambda l: np.round(l,1)).shift(delta)\n",
    "    \n",
    "    daily_data = daily_data.reset_index()\n",
    "    daily_data.loc[daily_data.Week >= (week+delta),\n",
    "                   \"QuantityProductTypeWeekWeekdayLag_\" + str(delta) + \"_median\"] = np.nan\n",
    "    \n",
    "data[\"ProductType\"] = data.StockCode.map(products.cluster)\n",
    "daily_data[\"TransactionsPerProductType\"] = daily_data.ProductType.map(data.loc[data.Week < week].groupby(\"ProductType\").InvoiceNo.nunique())\n",
    "### About countries and customers\n",
    "\n",
    "\n",
    "delta = 1\n",
    "to_group = [\"Week\", \"Weekday\", \"ProductType\"]\n",
    "daily_data = daily_data.set_index(to_group)\n",
    "daily_data[\"DummyWeekWeekdayAttraction\"] = data.groupby(to_group).CustomerID.nunique()\n",
    "daily_data[\"DummyWeekWeekdayMeanUnitPrice\"] = data.groupby(to_group).UnitPrice.mean().apply(lambda l: np.round(l, 2))\n",
    "\n",
    "daily_data[\"WeekWeekdayAttraction_Lag1\"] = daily_data[\"DummyWeekWeekdayAttraction\"].shift(1)\n",
    "daily_data[\"WeekWeekdayMeanUnitPrice_Lag1\"] = daily_data[\"DummyWeekWeekdayMeanUnitPrice\"].shift(1)\n",
    "\n",
    "daily_data = daily_data.reset_index()\n",
    "daily_data.loc[daily_data.Week >= (week + delta), \"WeekWeekdayAttraction_Lag1\"] = np.nan\n",
    "daily_data.loc[daily_data.Week >= (week + delta), \"WeekWeekdayMeanUnitPrice_Lag1\"] = np.nan\n",
    "daily_data = daily_data.drop([\"DummyWeekWeekdayAttraction\", \"DummyWeekWeekdayMeanUnitPrice\"], axis=1)\n",
    "daily_data[\"TransactionsPerStockCode\"] = daily_data.StockCode.map(\n",
    "    data.loc[data.Week < week].groupby(\"StockCode\").InvoiceNo.nunique())\n",
    "daily_data.head()\n",
    "daily_data[\"CustomersPerWeekday\"] = daily_data.Month.map(\n",
    "    data.loc[data.Week < week].groupby(\"Weekday\").CustomerID.nunique())\n",
    "X = daily_data.drop([\"Quantity\", \"Revenue\", \"Date\", \"Year\"], axis=1)\n",
    "y = daily_data.Quantity\n",
    "params = CatHyperparameter()\n",
    "\n",
    "model = Catmodel(\"new_features_1\", params)\n",
    "model.set_data(X,y, week)\n",
    "model.learn(plot=True)\n",
    "model.score()\n",
    "model.show_importances(kind=None)\n",
    "model.show_val_results();\n",
    "np.mean(np.abs(np.exp(model.results.prediction) - np.exp(model.results.Quantity)))\n",
    "np.median(np.abs(np.exp(model.results.prediction) - np.exp(model.results.Quantity)))\n",
    "search = Hypertuner(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
